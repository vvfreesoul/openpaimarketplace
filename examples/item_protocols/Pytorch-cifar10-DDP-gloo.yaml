protocolVersion: 2
name: Pytorch-cifar10-DDP-gloo
type: job
jobRetryCount: 0
description: |
  # Pytorch DDP Example

  This example shows how to train a custom neural network on cifar10 with Pytorch on OpenPAI. In this example, we use the DistributedDataParallel.

  DistributedDataParallel (DDP) implements data parallelism at the module level which can run across multiple machines. Applications using DDP should spawn multiple processes and create a single DDP instance per process.

  This example can be run on single-node or on multi-node, using Gloo backend for distributed GPU training.If you encounter any problem with NCCL, you can use Gloo as the fallback option. (Note that Gloo currently runs slower than NCCL for GPUs.)


prerequisites:
  - type: dockerimage
    uri: 'openpai/standard:python_3.6-pytorch_1.2.0-gpu'
    name: docker_image_0
taskRoles:
  worker:
    instances: 2
    completion:
      minFailedInstances: 1
    taskRetryCount: 0
    dockerImage: docker_image_0
    resourcePerInstance:
      gpu: 2
      cpu: 8
      memoryMB: 16384
      ports:
        SynPort: 1
    commands:
      - export GLOO_SOCKET_IFNAME=eth0
      - 'git clone https://github.com/NVIDIA/apex'
      - cd apex
      - python setup.py install
      - cd ..
      - >-
        wget
        https://raw.githubusercontent.com/microsoft/pai/master/examples/Distributed-example/cifar10-single-mul-DDP-nccl-gloo.py
      - >-
        python  cifar10-single-mul-DDP-nccl-gloo.py -n 2 -g 2 --epochs 2
        --dist-backend gloo

defaults:
  virtualCluster: default
extras:
  com.microsoft.pai.runtimeplugin:
    - plugin: ssh
      parameters:
        jobssh: true
        userssh: {}
  hivedScheduler:
    taskRoles:
      worker:
        skuNum: 2
        skuType: null
